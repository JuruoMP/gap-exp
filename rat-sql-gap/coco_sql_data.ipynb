{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "queries = []\n",
    "labels = []\n",
    "sqls = []\n",
    "db_ids = []\n",
    "with open(\"sparc/train.json\",\"r\") as f:\n",
    "    interactions = json.load(f)\n",
    "    for interaction in interactions:\n",
    "        db_id = interaction[\"database_id\"]\n",
    "        turns = interaction[\"interaction\"]\n",
    "        if len(turns)>1:\n",
    "            for i in range(1,len(turns)):\n",
    "                queries.append(\" <s> \".join([ele[\"utterance\"] for ele in turns[0:i]]))\n",
    "                sqls.append(turns[i][\"sql\"])\n",
    "                labels.append(turns[i][\"utterance\"])\n",
    "                db_ids.append(db_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import attr\n",
    "import torch\n",
    "from seq2struct.utils import registry\n",
    "\n",
    "@attr.s\n",
    "class PreprocessConfig:\n",
    "    config = attr.ib()\n",
    "    config_args = attr.ib()\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model_preproc = registry.instantiate(\n",
    "            registry.lookup('model', config['model']).Preproc,\n",
    "            config['model'])\n",
    "@attr.s\n",
    "class InferConfig:\n",
    "    config = attr.ib()\n",
    "    config_args = attr.ib()\n",
    "    logdir = attr.ib()\n",
    "    section = attr.ib()\n",
    "    beam_size = attr.ib()\n",
    "    output = attr.ib()\n",
    "    step = attr.ib()\n",
    "    use_heuristic = attr.ib(default=False)\n",
    "    mode = attr.ib(default=\"infer\")\n",
    "    limit = attr.ib(default=None)\n",
    "    output_history = attr.ib(default=False)\n",
    "\n",
    "\n",
    "class Inferer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            torch.set_num_threads(1)\n",
    "\n",
    "        # 0. Construct preprocessors\n",
    "        self.model_preproc = registry.instantiate(\n",
    "            registry.lookup('model', config['model']).Preproc,\n",
    "            config['model'])\n",
    "        self.model_preproc.load()\n",
    "\n",
    "    def load_model(self, logdir, step):\n",
    "        '''Load a model (identified by the config used for construction) and return it'''\n",
    "        # 1. Construct model\n",
    "        model = registry.construct('model', self.config['model'], preproc=self.model_preproc, device=self.device)\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        model.visualize_flag = False\n",
    "\n",
    "        # 2. Restore its parameters\n",
    "        saver = saver_mod.Saver({\"model\": model})\n",
    "        last_step = saver.restore(logdir, step=step, map_location=self.device, item_keys=[\"model\"])\n",
    "\n",
    "        if not last_step:\n",
    "            raise Exception('Attempting to infer on untrained model')\n",
    "        return model\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'seq2struct.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n"
     ]
    }
   ],
   "source": [
    "import _jsonnet\n",
    "from seq2struct import datasets\n",
    "from seq2struct import models\n",
    "from seq2struct.utils import registry\n",
    "from seq2struct.utils import vocab\n",
    "\n",
    "exp_config = json.loads(_jsonnet.evaluate_file(\"experiments/sparc-configs/gap-run.jsonnet\"))\n",
    "model_config_file = exp_config[\"model_config\"]\n",
    "model_config_args = json.dumps(exp_config[\"model_config_args\"])\n",
    "preprocess_config = PreprocessConfig(model_config_file, model_config_args)\n",
    "\n",
    "config = json.loads(_jsonnet.evaluate_file(preprocess_config.config, tla_codes={'args': preprocess_config.config_args}))\n",
    "\n",
    "preprocessor = Preprocessor(config)\n",
    "\n",
    "data = registry.construct('dataset', config['data'][\"train\"])\n",
    "\n",
    "# test = preprocessor.model_preproc.dec_preproc.grammar.parse(sqls[0],\"train\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "==========================="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'seq2struct.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "file data/sparc-bart-final/sparc,nl2code-1115,output_from=true,fs=2,emb=bart,cvlink/enc/config.json not found\n",
      "WARNING <class 'seq2struct.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': True, 'infer_from_conditions': True, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/sparc-bart-final/sparc,nl2code-1115,output_from=true,fs=2,emb=bart,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bart_version': 'facebook/bart-large', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/sparc-bart-final/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/sparc-bart-final/sparc,nl2code-1115,output_from=true,fs=2,emb=bart,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sparc-bart-final/sparc,nl2code-1115,output_from=true,fs=2,emb=bart,cvlink/enc\n",
      "Parameter containing:\n",
      "tensor([[-0.0370,  0.1117,  0.1829,  ...,  0.2054,  0.0578, -0.0750],\n",
      "        [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087],\n",
      "        [-0.0448,  0.4604, -0.0604,  ...,  0.1073,  0.0310,  0.0477],\n",
      "        ...,\n",
      "        [-0.0138,  0.0278, -0.0467,  ...,  0.0455, -0.0265,  0.0125],\n",
      "        [-0.0043,  0.0153, -0.0567,  ...,  0.0496,  0.0108, -0.0099],\n",
      "        [ 0.0053,  0.0324, -0.0179,  ..., -0.0085,  0.0223, -0.0020]],\n",
      "       requires_grad=True)\n",
      "Updated the model with ./pretrained_checkpoint/pytorch_model.bin\n",
      "Parameter containing:\n",
      "tensor([[-3.8313e-02,  1.2050e-01,  1.7760e-01,  ...,  1.9729e-01,\n",
      "          5.9443e-02, -6.9929e-02],\n",
      "        [ 4.5650e-03, -2.3032e-03, -8.4326e-03,  ..., -3.5686e-03,\n",
      "          4.7121e-03,  8.4110e-03],\n",
      "        [-4.5997e-02,  4.6710e-01, -6.5000e-02,  ...,  1.0271e-01,\n",
      "          2.5631e-02,  4.7501e-02],\n",
      "        ...,\n",
      "        [-2.5866e-03, -3.6627e-03, -1.1304e-02,  ..., -2.8429e-02,\n",
      "         -6.1576e-03,  3.8620e-02],\n",
      "        [ 9.2276e-03, -1.8064e-02, -5.2847e-03,  ...,  2.0911e-03,\n",
      "         -4.1957e-03, -3.1116e-02],\n",
      "        [-1.4816e-02, -2.0663e-02, -2.7307e-04,  ...,  8.6651e-03,\n",
      "         -3.7476e-02,  3.1683e-02]], requires_grad=True)\n",
      "Loading model from logdir/sparc_bart_run_1/bs=sparc,12,lr=1.0e-04,bert_lr=1.0e-05,end_lr=0e0,att=1/model_checkpoint-00038100\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from seq2struct import beam_search\n",
    "from seq2struct import datasets\n",
    "from seq2struct import models\n",
    "from seq2struct import optimizers\n",
    "from seq2struct.utils import registry\n",
    "from seq2struct.utils import saver as saver_mod\n",
    "\n",
    "from seq2struct.models.spider import spider_beam_search\n",
    "\n",
    "exp_config = json.loads(_jsonnet.evaluate_file(\"experiments/sparc-configs/gap-run.jsonnet\"))\n",
    "model_config_file = exp_config[\"model_config\"]\n",
    "model_config_args = json.dumps(exp_config[\"model_config_args\"])\n",
    "\n",
    "infer_output_path = \"{}/{}-step{}.infer\".format(\n",
    "                exp_config[\"eval_output\"],\n",
    "                exp_config[\"eval_name\"],\n",
    "                38100)\n",
    "\n",
    "infer_config = InferConfig(\n",
    "                model_config_file,\n",
    "                model_config_args,\n",
    "                exp_config[\"logdir\"],\n",
    "                exp_config[\"eval_section\"],\n",
    "                exp_config[\"eval_beam_size\"],\n",
    "                infer_output_path,\n",
    "                38100,\n",
    "                use_heuristic=exp_config[\"eval_use_heuristic\"]\n",
    "            )\n",
    "\n",
    "if infer_config.config_args:\n",
    "    config = json.loads(_jsonnet.evaluate_file(infer_config.config, tla_codes={'args': infer_config.config_args}))\n",
    "else:\n",
    "    config = json.loads(_jsonnet.evaluate_file(infer_config.config))\n",
    "\n",
    "if 'model_name' in config:\n",
    "    infer_config.logdir = os.path.join(infer_config.logdir, config['model_name'])\n",
    "\n",
    "output_path = infer_config.output.replace('__LOGDIR__', infer_config.logdir)\n",
    "\n",
    "\n",
    "inferer = Inferer(config)\n",
    "model = inferer.load_model(infer_config.logdir, infer_config.step)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 重写sqls\n",
    "class TTT:\n",
    "    def __init__(self,schema):\n",
    "        self.schema = schema\n",
    "rewrite_sqls = []\n",
    "for i in range(len(sqls)):\n",
    "    root = preprocessor.model_preproc.dec_preproc.grammar.parse(sqls[i],\"train\")\n",
    "    ttt = TTT(data.schemas[db_ids[i]])\n",
    "    r_sql = model.decoder.preproc.grammar.unparse(root, ttt)\n",
    "    rewrite_sqls.append(r_sql)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "with open(\"coco/sparc/train.json\",\"w\") as f:\n",
    "    for i in range(len(sqls)):\n",
    "        x = queries[i]+\" </s> \"+rewrite_sqls[i]\n",
    "        label = labels[i]\n",
    "        f.write(json.dumps({\"x\":x,\"label\":label})+\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "==============\n",
    "数据增强\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5995/5995 [00:25<00:00, 236.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from random import choice\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "def do_change_select_agg(sql,schema):\n",
    "    is_change = False\n",
    "    selects = sql[\"select\"][1]\n",
    "    is_distinct = sql[\"select\"][0]\n",
    "    for select in selects:\n",
    "        val_unit = select[1]\n",
    "        col_unit = val_unit[1]\n",
    "        if not is_distinct:\n",
    "            if col_unit[2]==False:\n",
    "                #no distinct\n",
    "                agg = col_unit[0]\n",
    "                column_id = col_unit[1]\n",
    "                #=0, aka *, skip\n",
    "                if column_id!=0:\n",
    "                    tp = schema.columns[column_id].type\n",
    "                    if tp=='text':\n",
    "                        agg_candidates =[0,3]\n",
    "                        if agg in agg_candidates:\n",
    "                            agg_candidates.remove(agg)\n",
    "                        new_agg = choice(agg_candidates)\n",
    "                        col_unit[0] = new_agg\n",
    "                        is_change = True\n",
    "    return is_change\n",
    "\n",
    "def do_change_select_column(sql,schema):\n",
    "    is_change = False\n",
    "    selects = sql[\"select\"][1]\n",
    "    is_distinct = sql[\"select\"][0]\n",
    "    for select in selects:\n",
    "        val_unit = select[1]\n",
    "        col_unit = val_unit[1]\n",
    "\n",
    "        if (not is_distinct) and col_unit[2]==False:\n",
    "            #no distinct\n",
    "            column_id = col_unit[1]\n",
    "\n",
    "            if column_id!=0:\n",
    "                tp = schema.columns[column_id].type\n",
    "                to_replaces = [ele.id for ele in schema.columns[column_id].table.columns if tp==ele.type]\n",
    "                if column_id in to_replaces:\n",
    "                    to_replaces.remove(column_id)\n",
    "                if 0 in to_replaces:\n",
    "                    to_replaces.remove(0)\n",
    "\n",
    "                if len(to_replaces)>0:\n",
    "                    to_replace = choice(to_replaces)\n",
    "                    col_unit[1]=to_replace\n",
    "                    is_change = True\n",
    "            else:\n",
    "                table_id = None\n",
    "                for table_unit in sql[\"from\"][\"table_units\"]:\n",
    "                    if table_unit[0] ==\"table_unit\":\n",
    "                        table_id = table_unit[1]\n",
    "                if table_id is not None:\n",
    "                    to_replaces = [ele.id for ele in schema.tables[table_id].columns if \"text\"==ele.type]\n",
    "                    if 0 in to_replaces:\n",
    "                        to_replaces.remove(0)\n",
    "                    if len(to_replaces)>0:\n",
    "                        to_replace = choice(to_replaces)\n",
    "                        col_unit[1]=to_replace\n",
    "                        is_change = True\n",
    "    return is_change\n",
    "\n",
    "def do_change_where_column(sql,schema):\n",
    "    #建立sqlite链接\n",
    "    conn = sqlite3.connect('sparc/database/{0}/{0}.sqlite'.format(schema.db_id))\n",
    "    cursor = conn.cursor()\n",
    "    is_change = False\n",
    "    wheres = sql[\"where\"]\n",
    "    for where in wheres:\n",
    "        if isinstance(where,list):\n",
    "            cond_unit = where\n",
    "            column_id = cond_unit[2][1][1]\n",
    "            tp = schema.columns[column_id].type\n",
    "            if tp == \"number\":\n",
    "                to_replaces = [ele.id for ele in schema.columns[column_id].table.columns if ele.type==tp]\n",
    "                if 0 in to_replaces:\n",
    "                    to_replaces.remove(0)\n",
    "                    if len(to_replaces)>0:\n",
    "                        cond_unit[2][1][1] = choice(to_replaces)\n",
    "                        is_change = True\n",
    "            if tp == \"text\":\n",
    "                to_replaces = [ele.id for ele in schema.columns[column_id].table.columns if ele.type==tp]\n",
    "                if column_id in to_replaces:\n",
    "                    to_replaces.remove(column_id)\n",
    "                if 0 in to_replaces:\n",
    "                    to_replaces.remove(0)\n",
    "                if len(to_replaces)>0:\n",
    "                    to_replace = choice(to_replaces)\n",
    "                    #随机选取\n",
    "                    try:\n",
    "                        cursor.execute(\"select {} from {} ORDER BY RANDOM() limit 2\".format(schema.columns[to_replace].orig_name,schema.columns[to_replace].table.orig_name))\n",
    "                    except:\n",
    "                        return False\n",
    "                    c_result = cursor.fetchall()\n",
    "                    vals = [ele[0] for ele in c_result]\n",
    "                    if vals is not None and len(vals) > 0:\n",
    "                        if not isinstance(cond_unit[3],dict):\n",
    "                            orig_val = cond_unit[3]\n",
    "                            if orig_val is None:\n",
    "                                return False\n",
    "                            if isinstance(orig_val,str):\n",
    "                                if len(orig_val)>0 and orig_val[0] == \"\\\"\":\n",
    "                                    orig_val = orig_val[1:]\n",
    "                                if len(orig_val)>0 and orig_val[-1] == \"\\\"\":\n",
    "                                    orig_val = orig_val[:-1]\n",
    "                            if orig_val in vals:\n",
    "                                vals = vals.remove(orig_val)\n",
    "                            if vals is not None and len(vals)>0:\n",
    "                                v_to_replace = choice(vals)\n",
    "                                cond_unit[2][1][1] = to_replace\n",
    "                                cond_unit[3] = v_to_replace\n",
    "                                is_change=True\n",
    "    return is_change\n",
    "\n",
    "rewrite_sqls = []\n",
    "for i in tqdm(range(len(sqls))):\n",
    "    _cur = {}\n",
    "\n",
    "    root = preprocessor.model_preproc.dec_preproc.grammar.parse(sqls[i],\"dev\")\n",
    "    ttt = TTT(data.schemas[db_ids[i]])\n",
    "    gold_sql = model.decoder.preproc.grammar.unparse(root, ttt)\n",
    "    _cur[\"gold\"] = gold_sql\n",
    "    # _cur[\"select_agg\"] = []\n",
    "    # _cur[\"select_agg_ast\"] = []\n",
    "    _cur[\"select_column\"] = []\n",
    "    _cur[\"select_column_ast\"] = []\n",
    "    _cur[\"where_column\"] = []\n",
    "    _cur[\"where_column_ast\"] = []\n",
    "\n",
    "\n",
    "    #5次select agg changed\n",
    "    # for iii in range(5):\n",
    "    #     changed_sql = copy.deepcopy(sqls[i])\n",
    "    #     is_changed = do_change_select_agg(changed_sql,data.schemas[db_ids[i]])\n",
    "    #     if is_changed:\n",
    "    #         root = preprocessor.model_preproc.dec_preproc.grammar.parse(changed_sql,\"dev\")\n",
    "    #         ttt = TTT(data.schemas[db_ids[i]])\n",
    "    #         r_sql = model.decoder.preproc.grammar.unparse(root, ttt)\n",
    "    #         _cur[\"select_agg\"].append(r_sql)\n",
    "    #         _cur[\"select_agg_ast\"].append(changed_sql)\n",
    "    for iii in range(5):\n",
    "        changed_sql = copy.deepcopy(sqls[i])\n",
    "        is_changed = do_change_select_column(changed_sql,data.schemas[db_ids[i]])\n",
    "        if is_changed:\n",
    "            root = preprocessor.model_preproc.dec_preproc.grammar.parse(changed_sql,\"dev\")\n",
    "            ttt = TTT(data.schemas[db_ids[i]])\n",
    "            r_sql = model.decoder.preproc.grammar.unparse(root, ttt)\n",
    "            _cur[\"select_column\"].append(r_sql)\n",
    "            _cur[\"select_column_ast\"].append(changed_sql)\n",
    "    for iii in range(10):\n",
    "        changed_sql = copy.deepcopy(sqls[i])\n",
    "        is_changed = do_change_where_column(changed_sql,data.schemas[db_ids[i]])\n",
    "        if is_changed:\n",
    "            try:\n",
    "                root = preprocessor.model_preproc.dec_preproc.grammar.parse(changed_sql,\"dev\")\n",
    "                ttt = TTT(data.schemas[db_ids[i]])\n",
    "                r_sql = model.decoder.preproc.grammar.unparse(root, ttt)\n",
    "                _cur[\"where_column\"].append(r_sql)\n",
    "                _cur[\"where_column_ast\"].append(changed_sql)\n",
    "            except:\n",
    "                ignored = 0\n",
    "    rewrite_sqls.append(_cur)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "with open(\"coco/sparc/train_aug.json\",\"w\") as f:\n",
    "    for i in range(len(sqls)):\n",
    "        record = {\n",
    "            \"db_id\":db_ids[i],\n",
    "            \"query\":queries[i],\n",
    "            \"gold_sql\":rewrite_sqls[i][\"gold\"],\n",
    "            \"rewrite_sqls\":{\n",
    "                # \"select_agg\" : rewrite_sqls[i][\"select_agg\"],\n",
    "                # \"select_agg_ast\" : rewrite_sqls[i][\"select_agg_ast\"],\n",
    "                \"select_column\" : rewrite_sqls[i][\"select_column\"],\n",
    "                \"select_column_ast\" : rewrite_sqls[i][\"select_column_ast\"],\n",
    "                \"where_column\" : rewrite_sqls[i][\"where_column\"],\n",
    "                \"where_column_ast\" : rewrite_sqls[i][\"where_column_ast\"]\n",
    "            }\n",
    "        }\n",
    "        f.write(json.dumps(record)+\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}